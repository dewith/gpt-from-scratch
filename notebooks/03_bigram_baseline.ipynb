{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🐣 Baseline Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📚 Libraries\n",
    "Import libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📂 Data\n",
    "Load the data from local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load(\"data/train_data.pt\")\n",
    "val_data = torch.load(\"data/val_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/encoder_dict.json\", \"r\") as f:\n",
    "    encoder_dict = json.load(f)\n",
    "\n",
    "stoi = encoder_dict[\"stoi\"]\n",
    "itos = encoder_dict[\"itos\"]\n",
    "itos = {int(k): v for k, v in encoder_dict[\"itos\"].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🦮 Data batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4  # how many samples to process at once\n",
    "block_size = 8  # the context length\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"Generates a small batch of data of inputs x and targets y\"\"\"\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(integers: list, itos: dict = itos) -> str:\n",
    "    \"\"\"Decode list of integers to text.\"\"\"\n",
    "    return \"\".join([itos[i] for i in integers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤖 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads the logits for the next token\n",
    "        # from the lookup table\n",
    "        self.lookup = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and tarder are both of shape (batch_size, sequence_length)\n",
    "        # (B, T, C) (batch_size, sequence_length, vocab_size)\n",
    "        logits = self.lookup(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape  # (4, 8, 96)\n",
    "            logits_ = logits.view(B * T, C)  # reshape to (32, 96)\n",
    "            targets_ = targets.view(-1)  # reshape to (32)\n",
    "            loss = F.cross_entropy(logits_, targets_)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, length):\n",
    "        with torch.no_grad():\n",
    "            for _ in range(length):\n",
    "                # get the predictions for the all the tokens\n",
    "                logits, _ = self.forward(idx)  # (B, T, C)\n",
    "                # get the last token\n",
    "                logits = logits[:, -1, :]  # (B, C)\n",
    "                # apply softmax to get the probabilities\n",
    "                probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "                # sample the next token\n",
    "                next_token = torch.multinomial(probs, 1)  # (B, 1)\n",
    "                # append the next token to the sequence\n",
    "                idx = torch.cat([idx, next_token], dim=-1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "xb, yb = get_batch(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 96])\n",
      "tensor(4.9825, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(stoi)\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBOa,\tHes$Hj(yq>145f4lLq5.7Oad3]f5kwYXX3+.$NNs4\\$:z:{bLmbHbz=kxV\n",
      "\u001d4c\t*\u001fzqL0_^I+{)oS6t.Co wh;jgj>'zGL3\n"
     ]
    }
   ],
   "source": [
    "tokens = model.generate(torch.zeros((1, 1), dtype=torch.long), 100)\n",
    "tokens_decoded = decode(tokens.squeeze().tolist())\n",
    "print(tokens_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 4.912\n",
      "Step: 1000, Loss: 3.836\n",
      "Step: 2000, Loss: 2.960\n",
      "Step: 3000, Loss: 2.724\n",
      "Step: 4000, Loss: 2.561\n",
      "Step: 5000, Loss: 2.371\n",
      "Step: 6000, Loss: 2.379\n",
      "Step: 7000, Loss: 2.429\n",
      "Step: 8000, Loss: 2.304\n",
      "Step: 9000, Loss: 2.257\n",
      "Step: 10000, Loss: 2.334\n",
      "Step: 11000, Loss: 2.371\n",
      "Step: 12000, Loss: 2.310\n",
      "Step: 13000, Loss: 2.230\n",
      "Step: 14000, Loss: 2.277\n",
      "Step: 15000, Loss: 2.182\n",
      "Step: 16000, Loss: 2.365\n",
      "Step: 17000, Loss: 2.320\n",
      "Step: 18000, Loss: 2.177\n",
      "Step: 19000, Loss: 2.230\n",
      "Step: 20000, Loss: 2.323\n",
      "Step: 21000, Loss: 2.306\n",
      "Step: 22000, Loss: 2.201\n",
      "Step: 23000, Loss: 2.272\n",
      "Step: 24000, Loss: 2.287\n",
      "Step: 25000, Loss: 2.422\n",
      "Step: 26000, Loss: 2.217\n",
      "Step: 27000, Loss: 2.266\n",
      "Step: 28000, Loss: 2.407\n",
      "Step: 29000, Loss: 2.334\n",
      "Step: 30000, Loss: 2.275\n",
      "Step: 31000, Loss: 2.198\n",
      "Step: 32000, Loss: 2.261\n",
      "Step: 33000, Loss: 2.277\n",
      "Step: 34000, Loss: 2.277\n",
      "Step: 35000, Loss: 2.286\n",
      "Step: 36000, Loss: 2.360\n",
      "Step: 37000, Loss: 2.179\n",
      "Step: 38000, Loss: 2.284\n",
      "Step: 39000, Loss: 2.421\n",
      "Step: 40000, Loss: 2.073\n",
      "Step: 41000, Loss: 2.312\n",
      "Step: 42000, Loss: 2.288\n",
      "Step: 43000, Loss: 2.218\n",
      "Step: 44000, Loss: 2.204\n",
      "Step: 45000, Loss: 2.308\n",
      "Step: 46000, Loss: 2.418\n",
      "Step: 47000, Loss: 2.331\n",
      "Step: 48000, Loss: 2.324\n",
      "Step: 49000, Loss: 2.203\n",
      "Step: 50000, Loss: 2.268\n",
      "Step: 51000, Loss: 2.260\n",
      "Step: 52000, Loss: 2.298\n",
      "Step: 53000, Loss: 2.434\n",
      "Step: 54000, Loss: 2.289\n",
      "Step: 55000, Loss: 2.352\n",
      "Step: 56000, Loss: 2.210\n",
      "Step: 57000, Loss: 2.309\n",
      "Step: 58000, Loss: 2.295\n",
      "Step: 59000, Loss: 2.243\n",
      "Step: 60000, Loss: 2.252\n",
      "Step: 61000, Loss: 2.221\n",
      "Step: 62000, Loss: 2.289\n",
      "Step: 63000, Loss: 2.410\n",
      "Step: 64000, Loss: 2.179\n",
      "Step: 65000, Loss: 2.289\n",
      "Step: 66000, Loss: 2.179\n",
      "Step: 67000, Loss: 2.350\n",
      "Step: 68000, Loss: 2.251\n",
      "Step: 69000, Loss: 2.185\n",
      "Step: 70000, Loss: 2.252\n",
      "Step: 71000, Loss: 2.355\n",
      "Step: 72000, Loss: 2.285\n",
      "Step: 73000, Loss: 2.268\n",
      "Step: 74000, Loss: 2.247\n",
      "Step: 75000, Loss: 2.287\n",
      "Step: 76000, Loss: 2.244\n",
      "Step: 77000, Loss: 2.313\n",
      "Step: 78000, Loss: 2.330\n",
      "Step: 79000, Loss: 2.290\n",
      "Step: 80000, Loss: 2.231\n",
      "Step: 81000, Loss: 2.258\n",
      "Step: 82000, Loss: 2.385\n",
      "Step: 83000, Loss: 2.220\n",
      "Step: 84000, Loss: 2.277\n",
      "Step: 85000, Loss: 2.368\n",
      "Step: 86000, Loss: 2.226\n",
      "Step: 87000, Loss: 2.355\n",
      "Step: 88000, Loss: 2.184\n",
      "Step: 89000, Loss: 2.222\n",
      "Step: 90000, Loss: 2.373\n",
      "Step: 91000, Loss: 2.320\n",
      "Step: 92000, Loss: 2.245\n",
      "Step: 93000, Loss: 2.177\n",
      "Step: 94000, Loss: 2.190\n",
      "Step: 95000, Loss: 2.301\n",
      "Step: 96000, Loss: 2.330\n",
      "Step: 97000, Loss: 2.260\n",
      "Step: 98000, Loss: 2.259\n",
      "Step: 99000, Loss: 2.464\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(50000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    # forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if steps % 1000 == 0:\n",
    "        print(f\"Step: {steps}, Loss: {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ts en l ento de viosider ca ortes lasestecuil fa prdejo cus cu la dellion steca ecoresa dela cun ran sentona s dero roprmeser.gio l l 16471 los naden y deconacarono 1.a da 1901974 stosio. do a al de (0\n"
     ]
    }
   ],
   "source": [
    "tokens = model.generate(torch.zeros((1, 1), dtype=torch.long), 200)\n",
    "tokens_decoded = decode(tokens.squeeze().tolist())\n",
    "print(tokens_decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
