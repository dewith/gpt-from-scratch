{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš˜ Tranformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ“š Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/Users/dewith/Repos/gpt-from-scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from unidecode import unidecode\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, force=True)\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ¹ Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module provides utility functions for working with data.\n",
    "The `get_corpus_text` function is used to get the corpus text from a dataset.\n",
    "The `get_data_split` function is used to split the data into train and val sets.\n",
    "The `Tokenizer` class is used to encode and decode text.\n",
    "The `Batcher` class is used to generate batches of data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_corpus_text(local_path=None, hf_path=None, is_clean=False):\n",
    "    \"\"\"Get the corpus text\"\"\"\n",
    "    if not is_clean:\n",
    "        if local_path is None and hf_path is None:\n",
    "            raise ValueError(\"At least one of local or hf path must be passed.\")\n",
    "\n",
    "        if local_path is not None and hf_path is None:\n",
    "            corpus_df = pd.read_csv(local_path)\n",
    "            path = local_path\n",
    "        elif local_path is None and hf_path is not None:\n",
    "            dataset = load_dataset(hf_path)\n",
    "            corpus_df = dataset[\"train\"].to_pandas()\n",
    "            path = hf_path\n",
    "        else:\n",
    "            try:\n",
    "                corpus_df = pd.read_csv(local_path)\n",
    "                path = local_path\n",
    "            except Exception as e:  # pylint: disable=broad-except\n",
    "                LOGGER.warning(\"â”‚   â”œâ”€â”€ Local path raised exception:\")\n",
    "                LOGGER.error(\"â”‚   â”œâ”€â”€ \\t%s\", e)\n",
    "                LOGGER.warning(\"â”‚   â”œâ”€â”€ Loading from Hugging Face dataset.\")\n",
    "                dataset = load_dataset(hf_path)\n",
    "                corpus_df = dataset[\"train\"].to_pandas()\n",
    "                path = hf_path\n",
    "        LOGGER.info(\"â”‚   â”œâ”€â”€ Loaded from %s\", path)\n",
    "\n",
    "        corpus_text = \"\\n\".join(corpus_df[\"doc_text\"])\n",
    "        corpus_text_clean = corpus_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "        corpus_text_clean = re.sub(r\" +\", \" \", corpus_text_clean)\n",
    "        pat = r'[^\\w\\s!\"Â·$%&/()=?Â¿\\\\|@#+,\\.-^\\*;:_\\[\\]\\{\\} !Â¡Â¿?,\\.@#$%^&\\*]'\n",
    "        corpus_text_clean = re.sub(pat, \"\", corpus_text_clean)\n",
    "        corpus_text_clean = corpus_text_clean.lower()\n",
    "        corpus_text_clean = unidecode(corpus_text_clean)\n",
    "        LOGGER.info(\"â”‚   â”œâ”€â”€ Corpues cleaned and normalized\")\n",
    "\n",
    "        with open(\"data/02_primary/corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(corpus_text_clean)\n",
    "    else:\n",
    "        with open(local_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            corpus_text_clean = f.read()\n",
    "        LOGGER.info(\"â”‚   â”œâ”€â”€ Loaded already clean corpus from %s\", local_path)\n",
    "\n",
    "    return corpus_text_clean\n",
    "\n",
    "\n",
    "def get_data_split(corpus_text, tokenizer):\n",
    "    \"\"\"Get the data split into training and validation sets.\"\"\"\n",
    "    data = torch.tensor(tokenizer.encode(corpus_text), dtype=torch.long)\n",
    "    train_size = int(len(data) * 0.90)\n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:]\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Tokenizer class for encoding and decoding text.\"\"\"\n",
    "\n",
    "    def __init__(self, corpus_text):\n",
    "        stoi, itos = self._get_token_maps(corpus_text)\n",
    "        self.stoi = stoi\n",
    "        self.itos = itos\n",
    "        self.vocab_size = len(stoi)\n",
    "\n",
    "    def encode(self, text: str) -> list:\n",
    "        \"\"\"Encode text to integers.\"\"\"\n",
    "        return [self.stoi.get(char, self.stoi[\"[UNK]\"]) for char in text]\n",
    "\n",
    "    def decode(self, integers: list) -> str:\n",
    "        \"\"\"Decode list of integers to text.\"\"\"\n",
    "        return \"\".join([self.itos[i] for i in integers])\n",
    "\n",
    "    def _get_token_maps(self, corpus_text):\n",
    "        chars = sorted(list(set(corpus_text)))\n",
    "        itos = dict(enumerate(chars, start=0))\n",
    "        itos[max(itos) + 1] = \"[UNK]\"\n",
    "        stoi = {char: i for i, char in enumerate(chars, start=0)}\n",
    "        stoi[\"[UNK]\"] = max(stoi)\n",
    "        return stoi, itos\n",
    "\n",
    "\n",
    "class Batcher:\n",
    "    \"\"\"Batcher class for generating batches of data.\"\"\"\n",
    "\n",
    "    # pylint: disable=too-few-public-methods\n",
    "\n",
    "    def __init__(self, train_data, val_data, batch_size, block_size):\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.batch_size = batch_size\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def get_batch(self, split):\n",
    "        \"\"\"Generates a small batch of data of inputs x and targets y\"\"\"\n",
    "        data = self.train_data if split == \"train\" else self.val_data\n",
    "        ix = torch.randint(len(data) - self.block_size, (self.batch_size,))\n",
    "        x = torch.stack([data[i : i + self.block_size] for i in ix])\n",
    "        y = torch.stack([data[i + 1 : i + self.block_size + 1] for i in ix])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Transformer model for language modeling.\"\"\"\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"Attetion head.\"\"\"\n",
    "\n",
    "    # pylint: disable=too-few-public-methods\n",
    "\n",
    "    def __init__(self, block_size=8, num_embeds=64, head_size=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(num_embeds, head_size, bias=False)\n",
    "        self.query = nn.Linear(num_embeds, head_size, bias=False)\n",
    "        self.value = nn.Linear(num_embeds, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        _, t, _ = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # Compute the attention scores\n",
    "        scores = (q @ k.transpose(-2, -1)) / (self.head_size**0.5)\n",
    "        scores_masked = scores.masked_fill(self.tril[:t, :t] == 0, float(\"-inf\"))\n",
    "        weights = torch.softmax(scores_masked, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        # Compute the weighted sum of the values\n",
    "        output = weights @ v\n",
    "        return output\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of attention.\"\"\"\n",
    "\n",
    "    # pylint: disable=too-few-public-methods\n",
    "    # pylint: disable=too-many-arguments\n",
    "\n",
    "    def __init__(self, num_heads, block_size, num_embeds, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = [\n",
    "            Head(block_size, num_embeds, head_size, dropout) for _ in range(num_heads)\n",
    "        ]\n",
    "        self.proj = nn.Linear(num_embeds, num_embeds)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed forward network for the transformer.\"\"\"\n",
    "\n",
    "    # pylint: disable=too-few-public-methods\n",
    "\n",
    "    def __init__(self, num_embeds, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_embeds, 4 * num_embeds),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * num_embeds, num_embeds),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block. Communication followed by computation\"\"\"\n",
    "\n",
    "    # pylint: disable=too-few-public-methods\n",
    "    # pylint: disable=too-many-arguments\n",
    "\n",
    "    def __init__(self, num_heads, block_size, num_embeds, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(num_embeds)\n",
    "        self.attention_heads = MultiHeadAttention(\n",
    "            num_heads, block_size, num_embeds, head_size, dropout\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(num_embeds)\n",
    "        self.feed_forward = FeedForward(num_embeds, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        x = x + self.attention_heads(self.layer_norm1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer model for language modeling.\"\"\"\n",
    "\n",
    "    # pylint: disable=too-many-instance-attributes\n",
    "    # pylint: disable=too-many-arguments\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        num_embeds: int = 32,\n",
    "        block_size: int = 8,\n",
    "        num_heads: int = 4,\n",
    "        head_size: int = 8,\n",
    "        num_layers: int = 3,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_embeds = num_embeds\n",
    "        self.block_size = block_size\n",
    "        self.head_size = head_size\n",
    "        # Each token reads the logits for the next token from the lookup table\n",
    "        self.token_embed_table = nn.Embedding(vocab_size, num_embeds)\n",
    "        self.position_embed_table = nn.Embedding(block_size, num_embeds)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                Block(num_heads, block_size, num_embeds, head_size, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(num_embeds)\n",
    "        self.linear_head = nn.Linear(num_embeds, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        b, t = idx.shape\n",
    "\n",
    "        # idx and targets are both of shape (batch_size, sequence_length)\n",
    "        tok_emb = self.token_embed_table(idx)  # (B, T, E)\n",
    "        pos_emb = self.position_embed_table(torch.arange(t).to(idx.device))  # (T, E)\n",
    "        x = tok_emb + pos_emb  # (B, T, E)\n",
    "        x = self.blocks(x)  # (B, T, E)\n",
    "        x = self.layer_norm(x)  # (B, T, E)\n",
    "        logits = self.linear_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # (B, T, C) (batch_size, sequence_length, vocab_size)\n",
    "            b, t, c = logits.shape  # Shape (32, 8, 527)\n",
    "            logits_ = logits.view(b * t, c)  # Reshape to (256, 527)\n",
    "            targets_ = targets.view(-1)  # Reshape from (32, 8) to (256)\n",
    "            loss = F.cross_entropy(logits_, targets_)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, length):\n",
    "        \"\"\"Generate text using the model.\"\"\"\n",
    "        generated_sequence = idx.clone()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(length):\n",
    "                # Ensure idx does not exceed the block size\n",
    "                idx = idx[:, -self.block_size :]\n",
    "\n",
    "                # Get the predictions for the current set of tokens\n",
    "                logits, _ = self.forward(idx)  # (B, T, C)\n",
    "                logits = logits[:, -1, :]  # Get the last token's logits (B, C)\n",
    "                probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "                next_token = torch.multinomial(probs, 1)  # (B, 1)\n",
    "\n",
    "                # Append the next token to the generated sequence (B, T+1)\n",
    "                generated_sequence = torch.cat(\n",
    "                    tensors=[generated_sequence, next_token], dim=-1\n",
    "                )\n",
    "                # Append the next token to idx for the next iteration\n",
    "                idx = torch.cat([idx, next_token], dim=-1)\n",
    "        return generated_sequence\n",
    "\n",
    "    def viz(self, x, y):\n",
    "        \"\"\"Visualize the model.\"\"\"\n",
    "        viz = make_dot(\n",
    "            self(x, y)[1],\n",
    "            params=dict(self.named_parameters()),\n",
    "            show_attrs=False,\n",
    "            show_saved=False,\n",
    "        )\n",
    "        return viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module provides functions for evaluating the model. The `estimate_loss`\n",
    "function is used to estimate the loss of the model on the training and\n",
    "validation sets.\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, batcher, eval_iters):\n",
    "    \"\"\"Estimate the loss of the model on the training and validation sets.\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = batcher.get_batch(split)\n",
    "            _, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸš¤ Main Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training the transformer model with self attention\n",
      "INFO:__main__:â”œâ”€â”€ Loading the dataset\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Loaded already clean corpus from data/02_primary/corpus.txt\n",
      "INFO:__main__:â”‚   â””â”€â”€ Data tokenized and train/val splitted\n",
      "INFO:__main__:â”‚\n"
     ]
    }
   ],
   "source": [
    "LOGGER.info(\"Training the transformer model with self attention\")\n",
    "\n",
    "# Data\n",
    "LOGGER.info(\"â”œâ”€â”€ Loading the dataset\")\n",
    "try:\n",
    "    dataset_local_path = \"data/02_primary/corpus.txt\"\n",
    "    corpus = get_corpus_text(dataset_local_path, is_clean=True)\n",
    "except FileNotFoundError:\n",
    "    dataset_local_path = \"data/01_raw/secop_corpus.csv\"\n",
    "    dataset_hf_path = \"dewithsan/secop_corpus_clean\"\n",
    "    corpus = get_corpus_text(dataset_local_path, dataset_hf_path)\n",
    "tokenizer = Tokenizer(corpus)\n",
    "train_data, val_data = get_data_split(corpus, tokenizer)\n",
    "LOGGER.info(\"â”‚   â””â”€â”€ Data tokenized and train/val splitted\")\n",
    "LOGGER.info(\"â”‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:â”œâ”€â”€ Defining hyperparameters\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "LOGGER.info(\"â”œâ”€â”€ Defining hyperparameters\")\n",
    "batch_size = 16\n",
    "block_size = 24\n",
    "num_embeds = 80\n",
    "num_heads = 8\n",
    "head_size = num_embeds // num_heads\n",
    "num_layers = 3\n",
    "dropout = 0.2\n",
    "learning_rate = 1e-3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_steps = 5000\n",
    "step_loss_interval = 100\n",
    "eval_interval = 500\n",
    "eval_iters = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:â”œâ”€â”€ Defining the model\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Model created with vocab of 97\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Optimizer: AdamW with lr 0.001\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Device: cpu\n",
      "INFO:__main__:â”‚   â””â”€â”€ Batcher size: 16\n",
      "INFO:__main__:â”‚\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "LOGGER.info(\"â”œâ”€â”€ Defining the model\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = Transformer(\n",
    "    vocab_size, num_embeds, block_size, num_heads, head_size, num_layers, dropout\n",
    ")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "batcher = Batcher(train_data, val_data, batch_size, block_size)\n",
    "LOGGER.info(\"â”‚   â”œâ”€â”€ Model created with vocab of %s\", vocab_size)\n",
    "LOGGER.info(\"â”‚   â”œâ”€â”€ Optimizer: AdamW with lr %s\", learning_rate)\n",
    "LOGGER.info(\"â”‚   â”œâ”€â”€ Device: %s\", device)\n",
    "LOGGER.info(\"â”‚   â””â”€â”€ Batcher size: %s\", batch_size)\n",
    "LOGGER.info(\"â”‚\")\n",
    "\n",
    "if False:\n",
    "    LOGGER.info(\"â”œâ”€â”€ Visualizing the model\")\n",
    "    viz_path = \"data/04_models/transformer.png\"\n",
    "    x_viz, y_viz = batcher.get_batch(\"train\")\n",
    "    x_viz, y_viz = x_viz.to(device), y_viz.to(device)\n",
    "    model_viz = model.viz(x_viz, y_viz)\n",
    "    model_viz.render(\n",
    "        filename=viz_path.rsplit(\".\", maxsplit=1)[0],\n",
    "        format=viz_path.rsplit(\".\", maxsplit=1)[-1],\n",
    "        cleanup=True,\n",
    "    )\n",
    "    LOGGER.info(\"â”‚   â””â”€â”€ Model visualization saved at %s\", viz_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:â”œâ”€â”€ Training the model with 5000 steps\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 0 ~ Loss: 4.7092\n",
      "INFO:__main__:â”‚   â”‚   â”œâ”€â”€ Train loss: 4.4706\n",
      "INFO:__main__:â”‚   â”‚   â””â”€â”€ Val loss:   4.4671\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 100 ~ Loss: 2.5229\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 200 ~ Loss: 2.3175\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 300 ~ Loss: 2.2343\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 400 ~ Loss: 2.3830\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 500 ~ Loss: 2.2409\n",
      "INFO:__main__:â”‚   â”‚   â”œâ”€â”€ Train loss: 2.2866\n",
      "INFO:__main__:â”‚   â”‚   â””â”€â”€ Val loss:   2.2548\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 600 ~ Loss: 2.1925\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 700 ~ Loss: 2.4422\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 800 ~ Loss: 2.1084\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 900 ~ Loss: 2.1943\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 1000 ~ Loss: 2.3896\n",
      "INFO:__main__:â”‚   â”‚   â”œâ”€â”€ Train loss: 2.1996\n",
      "INFO:__main__:â”‚   â”‚   â””â”€â”€ Val loss:   2.2198\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 1100 ~ Loss: 2.1915\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 1200 ~ Loss: 2.1884\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 1300 ~ Loss: 2.1566\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 1400 ~ Loss: 2.1281\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 1500 ~ Loss: 1.9724\n",
      "INFO:__main__:â”‚   â”‚   â”œâ”€â”€ Train loss: 2.1465\n",
      "INFO:__main__:â”‚   â”‚   â””â”€â”€ Val loss:   2.1500\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 1600 ~ Loss: 2.1015\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 1700 ~ Loss: 1.9852\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 1800 ~ Loss: 1.9731\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 1900 ~ Loss: 2.1374\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 2000 ~ Loss: 2.1173\n",
      "INFO:__main__:â”‚   â”‚   â”œâ”€â”€ Train loss: 2.0763\n",
      "INFO:__main__:â”‚   â”‚   â””â”€â”€ Val loss:   2.0671\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 2100 ~ Loss: 2.1658\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 2200 ~ Loss: 2.1556\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 2300 ~ Loss: 2.0623\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 2400 ~ Loss: 2.0432\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 2500 ~ Loss: 1.9771\n",
      "INFO:__main__:â”‚   â”‚   â”œâ”€â”€ Train loss: 2.0195\n",
      "INFO:__main__:â”‚   â”‚   â””â”€â”€ Val loss:   2.0082\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 2600 ~ Loss: 1.8950\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 2700 ~ Loss: 1.9835\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 2800 ~ Loss: 2.0576\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 2900 ~ Loss: 1.9937\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 3000 ~ Loss: 2.1977\n",
      "INFO:__main__:â”‚   â”‚   â”œâ”€â”€ Train loss: 1.9600\n",
      "INFO:__main__:â”‚   â”‚   â””â”€â”€ Val loss:   1.9683\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 3100 ~ Loss: 2.1532\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 3200 ~ Loss: 2.1414\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 3300 ~ Loss: 2.1232\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 3400 ~ Loss: 1.9091\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 3500 ~ Loss: 1.8363\n",
      "INFO:__main__:â”‚   â”‚   â”œâ”€â”€ Train loss: 1.9355\n",
      "INFO:__main__:â”‚   â”‚   â””â”€â”€ Val loss:   1.9125\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 3600 ~ Loss: 2.1532\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 3700 ~ Loss: 1.9032\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 3800 ~ Loss: 2.1689\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 3900 ~ Loss: 1.8109\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 4000 ~ Loss: 2.2063\n",
      "INFO:__main__:â”‚   â”‚   â”œâ”€â”€ Train loss: 1.8778\n",
      "INFO:__main__:â”‚   â”‚   â””â”€â”€ Val loss:   1.8748\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 4100 ~ Loss: 1.9844\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 4200 ~ Loss: 1.8881\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 4300 ~ Loss: 1.8919\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 4400 ~ Loss: 2.0942\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 4500 ~ Loss: 1.9824\n",
      "INFO:__main__:â”‚   â”‚   â”œâ”€â”€ Train loss: 1.8699\n",
      "INFO:__main__:â”‚   â”‚   â””â”€â”€ Val loss:   1.8738\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 4600 ~ Loss: 1.9162\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 4700 ~ Loss: 1.8106\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 4800 ~ Loss: 1.7955\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 4900 ~ Loss: 2.1458\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Step 5000 ~ Loss: 1.9540\n",
      "INFO:__main__:â”‚   â”‚   â”œâ”€â”€ Train loss: 1.8395\n",
      "INFO:__main__:â”‚   â”‚   â””â”€â”€ Val loss:   1.8201\n",
      "INFO:__main__:â”‚   â””â”€â”€ Model training completed\n",
      "INFO:__main__:â”‚\n",
      "INFO:__main__:â”œâ”€â”€ Generating text\n",
      "INFO:__main__:â”‚   â”œâ”€â”€ Text generated\n",
      "INFO:__main__:â”‚   â””â”€â”€ us paade clucion de la intocivor: comijo ma), la se digunicio, o compedicion celu\n",
      "INFO:__main__:â”‚\n",
      "INFO:__main__:â””â”€â”€ Saving the model\n",
      "INFO:__main__:    â””â”€â”€ Model saved at data/04_models/transformer_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "LOGGER.info(\"â”œâ”€â”€ Training the model with %s steps\", max_steps)\n",
    "for step in range(max_steps + 1):\n",
    "    # Forward pass\n",
    "    xb, yb = batcher.get_batch(\"train\")\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    _, loss = model(xb, yb)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if step % step_loss_interval == 0:\n",
    "        loss_str = f\"{loss.item():.4f}\"\n",
    "        LOGGER.info(\"â”‚   â”œâ”€â”€ Step %s ~ Loss: %s\", step, loss_str)\n",
    "\n",
    "    # Evaluate the model and log the losses\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss(model, batcher, eval_iters)\n",
    "        train_loss = f\"{losses['train']:.4f}\"\n",
    "        val_loss = f\"{losses['val']:.4f}\"\n",
    "        LOGGER.info(\"â”‚   â”‚   â”œâ”€â”€ Train loss: %s\", train_loss)\n",
    "        LOGGER.info(\"â”‚   â”‚   â””â”€â”€ Val loss:   %s\", val_loss)\n",
    "\n",
    "LOGGER.info(\"â”‚   â””â”€â”€ Model training completed\")\n",
    "LOGGER.info(\"â”‚\")\n",
    "\n",
    "LOGGER.info(\"â”œâ”€â”€ Generating text\")\n",
    "context = torch.randint(tokenizer.vocab_size, (1, 1)).to(device)\n",
    "generated_tokens = model.generate(context, 80)\n",
    "generated_text = tokenizer.decode(generated_tokens[0].tolist())\n",
    "LOGGER.info(\"â”‚   â”œâ”€â”€ Text generated\")\n",
    "LOGGER.info(\"â”‚   â””â”€â”€ %s\", generated_text.replace(\"\\n\", \" \"))\n",
    "LOGGER.info(\"â”‚\")\n",
    "\n",
    "LOGGER.info(\"â””â”€â”€ Saving the model\")\n",
    "local_path = \"data/04_models/transformer_model.pth\"\n",
    "torch.save(model.state_dict(), local_path)\n",
    "LOGGER.info(\"    â””â”€â”€ Model saved at %s\", local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
