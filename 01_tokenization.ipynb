{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü¶ï Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìö Libraries\n",
    "Import libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Data\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# DL\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìÇ Data\n",
    "Load the data and take a look at the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"dewithsan/secop_corpus_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_doc</th>\n",
       "      <th>doc_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>266671326</td>\n",
       "      <td>SOLICITUD  CERTIFICACI√ìN  DE \\nINSUFICIENCIA ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321522708</td>\n",
       "      <td>ADENDA  P√°gina  1  \\n \\n ADENDA No. 1 \\n \\nPe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>291869951</td>\n",
       "      <td>\\n  \\n \\n \\nSISTEMA ESTRAT√âGICO DE TRANSPORTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>291901564</td>\n",
       "      <td>CERTIFICACION DE INSUFICIENCIA\\nVIGENTE\\nDESDE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>304566990</td>\n",
       "      <td>ANE XO Nro. 2 \\nOBLIGACIONES DE LA POLIC√çA NAC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_doc                                           doc_text\n",
       "0  266671326   SOLICITUD  CERTIFICACI√ìN  DE \\nINSUFICIENCIA ...\n",
       "1  321522708   ADENDA  P√°gina  1  \\n \\n ADENDA No. 1 \\n \\nPe...\n",
       "2  291869951   \\n  \\n \\n \\nSISTEMA ESTRAT√âGICO DE TRANSPORTE...\n",
       "3  291901564  CERTIFICACION DE INSUFICIENCIA\\nVIGENTE\\nDESDE...\n",
       "4  304566990  ANE XO Nro. 2 \\nOBLIGACIONES DE LA POLIC√çA NAC..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df = dataset[\"train\"].to_pandas()\n",
    "corpus_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîë Tokenizer\n",
    "Tokenize the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of corpus: 133,947,131 characters\n"
     ]
    }
   ],
   "source": [
    "corpus_text = \"\\n\".join(corpus_df[\"doc_text\"])\n",
    "print(f\"Length of corpus: {len(corpus_text):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra spaces\n",
    "corpus_text_clean = corpus_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "corpus_text_clean = re.sub(r\" +\", \" \", corpus_text_clean)\n",
    "\n",
    "# Normalization\n",
    "pat = r'[^\\w\\s!\"¬∑$%&/()=?¬ø\\\\|@#+,\\.-^\\*;:_\\[\\]\\{\\} !¬°¬ø?,\\.@#$%^&\\*]'\n",
    "corpus_text_clean = re.sub(pat, \"\", corpus_text_clean)\n",
    "corpus_text_clean = corpus_text_clean.lower()\n",
    "corpus_text_clean = unidecode(corpus_text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 96\n",
      "\t\n",
      "\u000b\f\u001c\u001d\u001e\u001f !\"#$%&'()*+,./0123456789:;<=>?@ABCDEFGHILMNOPRSTUVXYZ[\\]^_abcdefghijklmnopqrstuvwxyz{|}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(corpus_text_clean)))\n",
    "print(f\"Number of unique characters: {len(chars)}\")\n",
    "print(\"\".join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {char: i for i, char in enumerate(chars, start=1)}\n",
    "itos = {i: char for i, char in enumerate(chars, start=1)}\n",
    "itos[0] = \"[UNK]\"\n",
    "\n",
    "\n",
    "def encode(text: str, stoi: dict = stoi) -> list:\n",
    "    \"\"\"Encode text to a list of integers.\"\"\"\n",
    "    return [stoi[char] if char in stoi else 0 for char in text]\n",
    "\n",
    "\n",
    "def decode(integers: list, itos: dict = itos) -> str:\n",
    "    \"\"\"Decode list of integers to text.\"\"\"\n",
    "    return \"\".join([itos[i] for i in integers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 82, 79, 68, 21, 9, 80, 88, 81, 71, 82]\n",
      "Hola, mundo\n"
     ]
    }
   ],
   "source": [
    "hey = \"Hola, mundo\"\n",
    "print(encode(hey))\n",
    "print(decode(encode(hey)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(corpus_text_clean), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([127545940]) torch.int64\n",
      "tensor([ 9, 86, 82, 79, 76, 70, 76, 87, 88, 71,  9, 70, 72, 85, 87, 76, 73, 76,\n",
      "        70, 68, 70, 76, 82, 81,  9, 71, 72,  9, 76, 81, 86, 88, 73, 76, 70, 76,\n",
      "        72, 81, 70, 76, 68,  9, 82,  9, 76, 81, 72, 91, 76, 86, 87, 72, 81, 70,\n",
      "        76, 68,  9, 71, 72,  9, 83, 72, 85, 86, 82, 81, 68, 79,  9, 70, 82, 71,\n",
      "        76, 74, 82, 34,  9, 87, 68, 75, 73, 26, 30,  9, 89, 72, 85, 86, 76, 82,\n",
      "        81, 34,  9, 25,  9, 22, 24,  9, 83, 68])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 114,791,346 characters\n",
      "Validation size: 12,754,594 characters\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(data) * 0.90)\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]\n",
    "\n",
    "print(f\"Train size: {train_data.shape[0]:,} characters\")\n",
    "print(f\"Validation size: {val_data.shape[0]:,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data to disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_data, \"data/train_data.pt\")\n",
    "torch.save(val_data, \"data/val_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stoi and itos as json\n",
    "encoder_dict = {\"stoi\": stoi, \"itos\": itos}\n",
    "with open(\"data/encoder_dict.json\", \"w\") as f:\n",
    "    json.dump(encoder_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
